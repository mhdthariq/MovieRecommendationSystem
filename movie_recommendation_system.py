# -*- coding: utf-8 -*-
"""movie_recommendation_system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Yj_EEsWh-ixJ103UXvMMklhCKTGark0E

# Sistem Rekomendasi Film

Proyek ini akan mengembangkan sistem rekomendasi film dengan menggunakan dua pendekatan:
1. Content-based Filtering
2. Collaborative Filtering dengan Deep Learning (PyTorch)

Notebook ini akan menunjukkan proses lengkap mulai dari pengunduhan dataset, pemahaman data, persiapan data, pemodelan, hingga evaluasi model.

## 1. Download dan Import Dataset

Pada bagian ini, kita akan mengunduh dataset [MovieLens 100K](https://grouplens.org/datasets/movielens/100k/) yang umum digunakan untuk sistem rekomendasi. Dataset ini berisi 100.000 peringkat dari 943 pengguna pada 1.682 film.
"""

# Import library yang dibutuhkan
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import zipfile
import requests
from io import BytesIO
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

# Set random seeds untuk reproduksibilitas
np.random.seed(42)
torch.manual_seed(42)

# Cek ketersediaan CUDA untuk training
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Jika CUDA tersedia, set random seed untuk CUDA juga
if device.type == "cuda":
    torch.cuda.manual_seed(42)
    torch.cuda.manual_seed_all(42)
    print(f"CUDA tersedia: {torch.cuda.get_device_name(0)}")
    print(f"Jumlah CUDA devices: {torch.cuda.device_count()}")

# Download dataset MovieLens 100K
url = "https://files.grouplens.org/datasets/movielens/ml-100k.zip"
response = requests.get(url)
zipdata = BytesIO(response.content)

with zipfile.ZipFile(zipdata) as zip_ref:
    zip_ref.extractall("ml-100k")

print("Dataset berhasil diunduh dan diekstrak")

# Membaca file ratings.dat
ratings_file = os.path.join('ml-100k', 'ml-100k', 'u.data')
ratings_columns = ['user_id', 'movie_id', 'rating', 'timestamp']
ratings = pd.read_csv(ratings_file, sep='\t', names=ratings_columns, encoding='latin-1')

# Membaca file movies.dat
movies_file = os.path.join('ml-100k', 'ml-100k', 'u.item')
movies_columns = ['movie_id', 'title', 'release_date', 'video_release_date', 'IMDb_URL', 'unknown', 'Action', 'Adventure',
                 'Animation', 'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
                 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']
movies = pd.read_csv(movies_file, sep='|', names=movies_columns, encoding='latin-1')

# Membaca file users.dat (jika diperlukan)
users_file = os.path.join('ml-100k', 'ml-100k', 'u.user')
users_columns = ['user_id', 'age', 'gender', 'occupation', 'zip_code']
users = pd.read_csv(users_file, sep='|', names=users_columns, encoding='latin-1')

# Melihat beberapa baris dari setiap DataFrame
print("Data Rating:")
print(ratings.head())
print("\nData Film:")
print(movies.head())
print("\nData Pengguna:")
print(users.head())

"""## 2. Data Understanding

Pada bagian ini, kita akan melakukan exploratory data analysis untuk memahami dataset yang kita gunakan.
"""

# Mengecek jumlah baris dan kolom pada setiap DataFrame
print("Jumlah data ratings:", ratings.shape)
print("Jumlah data movies:", movies.shape)
print("Jumlah data users:", users.shape)

# Memeriksa nilai yang hilang
print("Nilai yang hilang pada dataset ratings:")
print(ratings.isnull().sum())
print("\nNilai yang hilang pada dataset movies:")
print(movies.isnull().sum())
print("\nNilai yang hilang pada dataset users:")
print(users.isnull().sum())

# Pemeriksaan data duplikat dalam dataset
print("Pemeriksaan Data Duplikat:")

# Memeriksa duplikat dalam dataset ratings
ratings_duplicates = ratings.duplicated().sum()
print(f"Jumlah data duplikat dalam dataset ratings: {ratings_duplicates}")

# Memeriksa duplikat berdasarkan user_id dan movie_id (kombinasi yang seharusnya unik)
user_movie_duplicates = ratings.duplicated(subset=['user_id', 'movie_id']).sum()
print(f"Jumlah data duplikat berdasarkan kombinasi user_id dan movie_id: {user_movie_duplicates}")

# Memeriksa duplikat dalam dataset movies berdasarkan movie_id
movies_duplicates = movies.duplicated(subset=['movie_id']).sum()
print(f"Jumlah data duplikat berdasarkan movie_id dalam dataset movies: {movies_duplicates}")

# Memeriksa duplikat dalam dataset users berdasarkan user_id
users_duplicates = users.duplicated(subset=['user_id']).sum()
print(f"Jumlah data duplikat berdasarkan user_id dalam dataset users: {users_duplicates}")

# Jika terdapat duplikat, tampilkan contoh data duplikat (misalnya untuk ratings)
if user_movie_duplicates > 0:
    print("\nContoh data duplikat dalam dataset ratings:")
    duplicated_pairs = ratings[ratings.duplicated(subset=['user_id', 'movie_id'], keep=False)]
    print(duplicated_pairs.sort_values(by=['user_id', 'movie_id']).head())

# Melihat statistik deskriptif dari dataset ratings
ratings.describe()

# Distribusi rating
plt.figure(figsize=(10, 6))
sns.countplot(x='rating', data=ratings)
plt.title('Distribusi Rating Film')
plt.xlabel('Rating')
plt.ylabel('Jumlah')
plt.show()

# Distribusi jumlah rating per film
movie_counts = ratings['movie_id'].value_counts()
plt.figure(figsize=(12, 6))
plt.hist(movie_counts, bins=50)
plt.title('Distribusi Jumlah Rating per Film')
plt.xlabel('Jumlah Rating')
plt.ylabel('Jumlah Film')
plt.show()

# Distribusi jumlah rating per user
user_counts = ratings['user_id'].value_counts()
plt.figure(figsize=(12, 6))
plt.hist(user_counts, bins=50)
plt.title('Distribusi Jumlah Rating per Pengguna')
plt.xlabel('Jumlah Rating')
plt.ylabel('Jumlah Pengguna')
plt.show()

# Distribusi genre film
genre_columns = movies.columns[5:]
genre_counts = movies[genre_columns].sum().sort_values(ascending=False)

plt.figure(figsize=(14, 8))
sns.barplot(x=genre_counts.index, y=genre_counts.values)
plt.title('Distribusi Genre Film')
plt.xlabel('Genre')
plt.ylabel('Jumlah Film')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Top 10 film dengan rating tertinggi (minimal 50 rating)
min_ratings = 50
movie_stats = ratings.groupby('movie_id').agg({'rating': ['count', 'mean']})
movie_stats.columns = ['count', 'mean']
popular_movies = movie_stats[movie_stats['count'] >= min_ratings].sort_values('mean', ascending=False)

top_movies = popular_movies.head(10).join(movies.set_index('movie_id'))[['title', 'count', 'mean']]
top_movies.reset_index(inplace=True)
top_movies.head(10)

"""## 3. Data Preparation

Pada bagian ini, kita akan mempersiapkan data untuk digunakan dalam pemodelan.
"""

# Membuat feature engineering untuk Content-Based Filtering
# Menggabungkan genre menjadi satu string untuk setiap film
def create_genre_string(row):
    genres = []
    for genre in genre_columns:
        if row[genre] == 1:
            genres.append(genre)
    return ' '.join(genres)

movies['genres'] = movies.apply(create_genre_string, axis=1)
movies.head()

# Persiapan data untuk Collaborative Filtering
# Kita akan membagi data menjadi training dan testing set
from sklearn.model_selection import train_test_split

# Normalisasi rating dari 1-5 menjadi 0-1 untuk memudahkan pelatihan model
ratings['normalized_rating'] = ratings['rating'] / 5.0

# Split data
train_data, test_data = train_test_split(ratings, test_size=0.2, random_state=42)

print(f"Jumlah data training: {len(train_data)}")
print(f"Jumlah data testing: {len(test_data)}")

"""## 4. Modeling

Pada bagian ini, kita akan mengimplementasikan dua pendekatan untuk sistem rekomendasi: Content-based Filtering dan Collaborative Filtering menggunakan Deep Learning dengan PyTorch.

### 4.1 Content-Based Filtering

Content-based filtering akan merekomendasikan film berdasarkan kemiripan konten (dalam hal ini genre).
"""

# Menggunakan TF-IDF untuk mengkonversi data genre menjadi vektor
tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(movies['genres'])
print("Dimensi TF-IDF matrix:", tfidf_matrix.shape)

# Menghitung kemiripan kosinus antar film
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
print("Dimensi similarity matrix:", cosine_sim.shape)

# Membuat indeks film dan reverse mapping
indices = pd.Series(movies.index, index=movies['title']).drop_duplicates()

# Fungsi untuk mendapatkan rekomendasi film berdasarkan judul film
def get_content_based_recommendations(title, cosine_sim=cosine_sim, movies=movies, indices=indices):
    # Dapatkan indeks film yang sesuai dengan judul
    try:
        idx = indices[title]
    except KeyError:
        print(f"Film dengan judul '{title}' tidak ditemukan.")
        return pd.DataFrame()

    # Dapatkan skor kemiripan dari semua film dengan film yang dipilih
    sim_scores = list(enumerate(cosine_sim[idx]))

    # Urutkan film berdasarkan skor kemiripan
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # Dapatkan skor dari 10 film yang paling mirip (tidak termasuk film itu sendiri)
    sim_scores = sim_scores[1:11]

    # Dapatkan indeks film
    movie_indices = [i[0] for i in sim_scores]
    similarity_scores = [i[1] for i in sim_scores]

    # Kembalikan 10 film yang paling mirip
    result = movies.iloc[movie_indices][['title', 'genres']].copy()
    result['similarity_score'] = similarity_scores
    return result

# Contoh penggunaan fungsi rekomendasi content-based
movie_title = "Toy Story (1995)"
content_recommendations = get_content_based_recommendations(movie_title)
print(f"Rekomendasi film untuk: {movie_title}\n")
content_recommendations

"""### 4.2 Collaborative Filtering dengan Deep Learning (PyTorch)

Collaborative filtering akan merekomendasikan film berdasarkan preferensi pengguna lain yang mirip. Kita akan mengimplementasikannya menggunakan model neural network dengan PyTorch.
"""

# Mendapatkan jumlah user dan movie untuk inisialisasi model
num_users = ratings['user_id'].nunique()
num_movies = ratings['movie_id'].nunique()
print(f"Jumlah users: {num_users}, Jumlah movies: {num_movies}")

# Membuat dataset dan dataloader PyTorch
class MovieLensDataset(Dataset):
    def __init__(self, df):
        self.df = df

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        user_id = self.df.iloc[idx]['user_id'] - 1  # Pengurangan 1 karena index dimulai dari 0 di PyTorch
        movie_id = self.df.iloc[idx]['movie_id'] - 1  # Pengurangan 1 untuk alasan yang sama
        rating = self.df.iloc[idx]['normalized_rating']

        return {'user_id': torch.tensor(user_id, dtype=torch.long),
                'movie_id': torch.tensor(movie_id, dtype=torch.long),
                'rating': torch.tensor(rating, dtype=torch.float)}

# Buat train dan test datasets
train_dataset = MovieLensDataset(train_data)
test_dataset = MovieLensDataset(test_data)

# Buat dataloaders
batch_size = 64
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size)

# Membuat model Collaborative Filtering menggunakan Neural Network
class NCF(nn.Module):
    def __init__(self, num_users, num_movies, embedding_size=50, hidden_layers=[100, 50]):
        super(NCF, self).__init__()

        # Embedding layers
        self.user_embedding = nn.Embedding(num_users, embedding_size)
        self.movie_embedding = nn.Embedding(num_movies, embedding_size)

        # MLP layers
        self.fc_layers = nn.ModuleList()
        input_size = embedding_size * 2  # Concatenated user and movie embeddings

        for i, hidden_size in enumerate(hidden_layers):
            self.fc_layers.append(nn.Linear(input_size, hidden_size))
            self.fc_layers.append(nn.ReLU())
            input_size = hidden_size

        # Output layer
        self.output_layer = nn.Linear(hidden_layers[-1], 1)
        self.sigmoid = nn.Sigmoid()

        # Initialize weights
        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Embedding):
                nn.init.normal_(m.weight, mean=0.0, std=0.01)
            elif isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, user_indices, movie_indices):
        user_embedded = self.user_embedding(user_indices)
        movie_embedded = self.movie_embedding(movie_indices)

        # Concatenate user and movie embeddings
        vector = torch.cat([user_embedded, movie_embedded], dim=-1)

        # Forward pass through MLP layers
        for layer in self.fc_layers:
            vector = layer(vector)

        # Output prediction
        prediction = self.output_layer(vector)
        prediction = self.sigmoid(prediction)

        return prediction.squeeze()

# Inisialisasi model, loss function, dan optimizer
model = NCF(num_users, num_movies, embedding_size=50, hidden_layers=[100, 50])
model = model.to(device)  # Pindahkan model ke GPU jika tersedia
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

print(f"Model dijalankan pada device: {next(model.parameters()).device}")

# Fungsi untuk melatih model
def train_model(model, train_loader, criterion, optimizer, device, epochs=10):
    model.train()
    train_losses = []

    for epoch in range(epochs):
        running_loss = 0.0
        for batch_idx, batch in enumerate(train_loader):
            # Pindahkan data ke device yang sama dengan model (CPU atau CUDA)
            user_id = batch['user_id'].to(device)
            movie_id = batch['movie_id'].to(device)
            rating = batch['rating'].to(device)

            # Forward pass
            optimizer.zero_grad()
            prediction = model(user_id, movie_id)

            # Compute loss
            loss = criterion(prediction, rating)

            # Backward pass dan optimisasi
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        avg_loss = running_loss / len(train_loader)
        train_losses.append(avg_loss)
        print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")

    return train_losses

# Melatih model
print("Mulai pelatihan model...")
train_losses = train_model(model, train_loader, criterion, optimizer, device, epochs=10)

# Plot proses pelatihan
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(train_losses) + 1), train_losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.tight_layout()
plt.show()

# Fungsi untuk evaluasi model
def evaluate_model(model, test_loader, device):
    model.eval()
    y_true = []
    y_pred = []

    with torch.no_grad():
        for batch in test_loader:
            # Pindahkan data ke device yang sama dengan model (CPU atau CUDA)
            user_id = batch['user_id'].to(device)
            movie_id = batch['movie_id'].to(device)
            rating = batch['rating']

            # Forward pass
            prediction = model(user_id, movie_id)

            # Pindahkan hasil prediksi ke CPU untuk evaluasi
            prediction = prediction.cpu()

            y_true.extend(rating.tolist())
            y_pred.extend(prediction.tolist())

    # Menghitung metrik evaluasi
    rmse = np.sqrt(mean_squared_error(y_true, y_pred)) * 5  # Kalikan dengan 5 untuk mendapatkan skala rating asli (1-5)
    mae = mean_absolute_error(y_true, y_pred) * 5

    # Menghitung metrik accuracy (menganggap prediksi yang berbeda kurang dari 0.5 sebagai benar)
    y_true_rounded = np.round(np.array(y_true) * 5) / 5  # Kembali ke skala 0-1
    y_pred_rounded = np.round(np.array(y_pred) * 5) / 5
    accuracy = np.mean(np.abs(y_true_rounded - y_pred_rounded) <= 0.1)

    return {
        'rmse': rmse,
        'mae': mae,
        'accuracy': accuracy
    }

# Evaluasi model
print("Evaluasi model...")
evaluation_metrics = evaluate_model(model, test_loader, device)
print(f"RMSE: {evaluation_metrics['rmse']:.4f}")
print(f"MAE: {evaluation_metrics['mae']:.4f}")
print(f"Accuracy: {evaluation_metrics['accuracy']:.4f}")

# Fungsi untuk mendapatkan rekomendasi colaborative filtering untuk seorang user
def get_collaborative_recommendations(user_id, model, movies_df, device, top_n=10):
    model.eval()

    # Dapatkan daftar film yang belum ditonton oleh user
    user_ratings = ratings[ratings['user_id'] == user_id]['movie_id'].unique()
    all_movies = movies_df['movie_id'].unique()
    movies_to_predict = np.setdiff1d(all_movies, user_ratings)

    # Buat prediksi untuk setiap film
    predictions = []

    for movie_id in movies_to_predict:
        with torch.no_grad():
            user_tensor = torch.tensor([user_id - 1], dtype=torch.long).to(device)
            movie_tensor = torch.tensor([movie_id - 1], dtype=torch.long).to(device)

            prediction = model(user_tensor, movie_tensor).item()
            predictions.append((movie_id, prediction * 5))  # Kembali ke skala 1-5

    # Urutkan prediksi
    predictions.sort(key=lambda x: x[1], reverse=True)

    # Dapatkan top-N film
    top_movies = []
    for movie_id, pred_rating in predictions[:top_n]:
        movie_info = movies_df[movies_df['movie_id'] == movie_id][['title', 'genres']].iloc[0]
        top_movies.append({
            'movie_id': movie_id,
            'title': movie_info['title'],
            'genres': movie_info['genres'],
            'predicted_rating': pred_rating
        })

    return pd.DataFrame(top_movies)

# Menampilkan rekomendasi collaborative filtering untuk user tertentu
user_id_example = 100  # Pilih user id untuk contoh
user_cf_recommendations = get_collaborative_recommendations(user_id_example, model, movies, device)

print(f"Rekomendasi film menggunakan Collaborative Filtering untuk User ID: {user_id_example}\n")
user_cf_recommendations

"""## 5. Evaluasi

Pada bagian ini, kita akan mengevaluasi model sistem rekomendasi yang telah dibangun.

### 5.1 Evaluasi Content-Based Filtering

Untuk content-based filtering, kita akan menggunakan metrik presisi (relevance) dan kesesuaian genre untuk evaluasi.
"""

# Evaluasi Content-Based Filtering dengan metrik precision
def evaluate_content_based_recommendations(title, recommendations):
    # Dapatkan genre film yang ditanyakan
    target_movie = movies[movies['title'] == title]
    if len(target_movie) == 0:
        print(f"Film dengan judul '{title}' tidak ditemukan.")
        return {}

    target_genres = target_movie['genres'].iloc[0].split()

    # Hitung presisi berdasarkan genre yang sama
    precisions = []
    for _, row in recommendations.iterrows():
        recommended_genres = row['genres'].split()
        common_genres = set(target_genres).intersection(set(recommended_genres))
        precision = len(common_genres) / len(target_genres) if target_genres else 0
        precisions.append(precision)

    avg_precision = np.mean(precisions) if precisions else 0
    coverage = len(np.unique(np.concatenate([r['genres'].split() for _, r in recommendations.iterrows()]))) / len(genre_columns)

    return {
        'avg_precision': avg_precision,
        'coverage': coverage
    }

# Evaluasi beberapa film contoh untuk Content-Based Filtering
example_movies = ["Star Wars (1977)", "Toy Story (1995)", "Pulp Fiction (1994)"]

content_based_results = {}
for movie in example_movies:
    recommendations = get_content_based_recommendations(movie)
    if not recommendations.empty:
        metrics = evaluate_content_based_recommendations(movie, recommendations)
        content_based_results[movie] = metrics
        print(f"Film: {movie}")
        print(f"Average Precision: {metrics['avg_precision']:.4f}")
        print(f"Coverage: {metrics['coverage']:.4f}")
        print("")

"""### 5.2 Evaluasi Collaborative Filtering

Untuk collaborative filtering, kita sudah mengevaluasi model dengan metrik RMSE, MAE, dan Accuracy yang umum digunakan untuk sistem rekomendasi. Mari kita tampilkan kembali hasilnya:
"""

# Menampilkan metrik evaluasi untuk Collaborative Filtering
print("Metrik Evaluasi untuk Collaborative Filtering:")
print(f"RMSE: {evaluation_metrics['rmse']:.4f}")
print(f"MAE: {evaluation_metrics['mae']:.4f}")
print(f"Accuracy: {evaluation_metrics['accuracy']:.4f}")

# Menggunakan hasil evaluasi yang sudah dihitung sebelumnya dari model

"""### 5.3 Perbandingan Metode

Mari kita bandingkan kedua metode rekomendasi berdasarkan kelebihan dan kekurangannya:

#### Content-Based Filtering

**Kelebihan**:
- Tidak memerlukan data dari pengguna lain, hanya butuh profil film
- Dapat merekomendasikan item yang baru atau tidak populer
- Dapat memberikan penjelasan tentang mengapa item direkomendasikan (transparansi)

**Kekurangan**:
- Terbatas pada fitur yang tersedia (dalam kasus ini hanya genre)
- Sulit menemukan preferensi baru yang tidak ada dalam profil pengguna
- Cenderung memberikan rekomendasi yang mirip saja (kurang eksplorasi)

#### Collaborative Filtering

**Kelebihan**:
- Dapat merekomendasikan item di luar profil pengguna berdasarkan kesamaan dengan pengguna lain
- Tidak memerlukan data konten untuk memberikan rekomendasi
- Dapat memberikan rekomendasi yang lebih bervariasi

**Kekurangan**:
- Memerlukan data rating dari banyak pengguna (cold start problem)
- Tidak bisa merekomendasikan item baru yang belum pernah dirating (item cold start)
- Dapat memiliki masalah sparsity ketika dataset ratings jarang dan kecil

### 5.4 Kesimpulan Evaluasi

Dari hasil evaluasi di atas, kita dapat menyimpulkan:

1. **Content-Based Filtering**: Model ini memberikan rekomendasi dengan rata-rata presisi genre yang cukup baik, artinya film yang direkomendasikan memiliki kemiripan genre dengan film yang ditanyakan. Hal ini sesuai dengan konsep content-based filtering yang fokus pada kemiripan konten.

2. **Collaborative Filtering**: Model ini memberikan hasil yang cukup baik dengan nilai RMSE 0.9699 dan MAE 0.7513 yang menunjukkan ketepatan prediksi rating, serta accuracy sebesar 42.52% yang menunjukkan proporsi prediksi yang mendekati rating asli. Setelah beberapa kali training dengan parameter yang sama, kita mendapatkan sedikit peningkatan dari hasil sebelumnya (RMSE 0.9763, Accuracy 42.52%), menunjukkan bahwa stabilitas training neural network memengaruhi hasil akhir.

3. **Perbandingan**: Kedua pendekatan memiliki kelebihan dan kelemahan masing-masing. Content-based filtering lebih baik untuk merekomendasikan item baru atau tidak populer, sementara collaborative filtering lebih baik untuk menemukan preferensi baru dari pengguna.

Untuk sistem rekomendasi yang lebih baik, kedua pendekatan ini dapat dikombinasikan dalam metode hybrid untuk menutupi kelemahan masing-masing.

## 6. Kesimpulan

Dalam proyek ini, kita telah berhasil mengembangkan sistem rekomendasi film menggunakan dua pendekatan berbeda: Content-Based Filtering dan Collaborative Filtering dengan Deep Learning menggunakan PyTorch.

**Content-Based Filtering** berhasil merekomendasikan film berdasarkan kemiripan genre, yang memungkinkan sistem memberikan rekomendasi bahkan untuk film yang belum pernah dinilai oleh pengguna. Pendekatan ini sangat berguna untuk pengguna baru atau film baru.

**Collaborative Filtering** dengan model neural network berhasil mempelajari pola tersembunyi dari interaksi pengguna-film dan memberikan rekomendasi yang dipersonalisasi berdasarkan preferensi pengguna lain yang serupa. Pendekatan ini mengatasi keterbatasan content-based dengan merekomendasikan film yang mungkin tidak mirip secara konten tetapi disukai oleh pengguna serupa.

Kedua pendekatan ini saling melengkapi dan dapat dikombinasikan untuk membuat sistem rekomendasi hybrid yang lebih kuat, yang dapat menangani berbagai situasi seperti cold-start problem dan memberikan rekomendasi yang lebih bervariasi dan akurat.
"""